---
layout: post
title: Google Summer of Code - Week 8-9
# bigimg: /img/gsoc-logo.png
tags: [gsoc, data-science, machine-learning]
---

These past weeks, I successfully completed my [first patch](https://github.com/olinguyen/shogun/commit/68517acd3cccc337e0bbbab59633eb41476fc567) to the Shogun toolbox. This patch allows `BinaryLabels` or `MulticlassLabels` to be initialized using a vector or matrix of confidence scores. This avoids having to set labels or scores again independently after creating the object, and ended up being useful for [another patch](https://github.com/shogun-toolbox/shogun/pull/3954) I am working on which will produce a probability output score for the Random Forest classifier. This is accomplished by using the predicted output labels from all the deciding trees and computing their weight/ratio. For instance, if a Random Forest classifier uses 10 trees to make a prediction, and 7 trees predicted the binary outcome of +1, then the probability of class 1 is 0.70.

With the latest patch, I evaluated the random forest classifier using the MIMIC database for mortality prediction and compared it to the previous Shogun classifiers. Random forest a slight improvement over logistic regression and support vector machines when observing the mean AUC across 10 folds.


| Classifier          | Mean AUC across 10 folds (%) |
|---------------------|---------------------------|
|                     | 30-day Hospital mortality |
| Logistic Regression | 84.64                     |
| Linear SVM          | 84.56                     |
| Random Forest       | 85.50                     |
| Random guess        | 50.00                     |        


![](/img/week9/roc-curve.png "ROC Curve")
![](/img/week9/error-plots.png "Error plots")

## Up Next

The patch to have probability outputs for Random Forest needs a few fixes in the unit tests before merging to the develop branch. From there, I'll be able to compute the AUC scores from my previous models and evaluate the Random Forest classifier with logistic regression and SVM. Additionally, I've began work on the data project using the temporal information of the MIMIC database. A prototype notebook of my work can be found [here](https://github.com/olinguyen/gsoc2017-shogun-dataproject/blob/master/Time-series.ipynb). In short, I've extracted the time series data of a patient's heart rate in the first 24 hours, which is quite significantly large (over a million data points for 32,000 patients). I've then created a training data set by computing the mean heart rate and with mortality prediction outcomes at different points in time (12 hours, 24 hours, 1 week, 1 month). So I now have a different (X, y) pairs at different points in time. From here, I will have to incorporate the training data retrospectively to make predictions of future time points like 1 week, 2 weeks and 1 month mortality prediction.