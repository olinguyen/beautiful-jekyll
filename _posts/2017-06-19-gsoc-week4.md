---
layout: post
title: Google Summer of Code - Week 4
# bigimg: /img/gsoc-logo.png
tags: [gsoc, data-science, machine-learning]
---

The code to follow this blog post can be viewed in [this notebook](https://github.com/olinguyen/gsoc2017-shogun-dataproject/blob/master/Feature%20Selection%20%26%20Dimensionality%20Reduction.ipynb).

In the [previous week](https://olinguyen.github.io/2017-06-12-gsoc-week3/), we extracted much more features from the MIMIC database to construct a more complete model for mortality prediction and predicting the hospital length of stay. This introduced the issues of much more NaN values being present in the data because not all lab measurements are recorded for every patient. To circumvent this, we will make use of the `pandas` library to deal with missing data. More specifically, the data imputation technique, or the method of replacement of the missing data, will employ mean substitution which will replace missing values with the mean value of that feature. Doing so allows us to increase our dataset size by 3,000, with a total of over 32,000 data points. Additionally, we shall add [data normalization](https://en.wikipedia.org/wiki/Feature_scaling ) to preprocess the data. Because our data has very different features that have different metrics, units and scales, we will standardize the data by making each feature have zero mean by subtracting the mean, and have unit-variance. Feature scaling ensures that all the data is normalized, that the features are in the same range. Some algorithms like the [SVM](https://en.wikipedia.org/wiki/Support_vector_machine) can converge faster on normalized data.

The next issue that was introduced by extracting much more features is often called the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). This concept refers the challenges that occur when working with data in increasing high-dimensional spaces (or features). In our case, we're trying to solve a problem in a 48-dimensional space. While certain problems can reach hundreds, even thousands of dimensions, it still remains a challenge to deal with increasing large inputs as it becomes harder to store, manipulate and compute with. For some algorithms like the SVM, increasing the number of dimensions can exponentially increase the compute time. For this reason, we will investigate dimensionality reduction techniques to reduce and simplify the problem.

## Principal Component Analysis (PCA)

[Principal component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) is a statistical technique that finds patterns in high-dimensional data and converts it into a set of linearly uncorrelated variables called principal components. This is particularly relevant as we will use PCA on patient data and visualize it in 2 dimensions; the first and second principal components that explain the most variance in the data. The PCA projection into 2D subspace can be seen below. While the survival and non-survival groups have overlapping points, we can still observe a pattern with 2 clusters in the plot.

![](/img/week4/pca-2d.png "PCA 2D Plot")

It is also possible to use more than 2 principal components of PCA. Below, we compute the AUC using different values of components and evaluate it with a logistic regression classifier.

| Number of Components | Logistic Regression AUC    |
|----------------------|--------|
| 2                    | 0.5126 |
| 4                    | 0.5392 |
| 8                    | 0.5526 |
| 10                   | 0.5486 |
| 16                   | 0.5664 |
| 20                   | 0.5698 |
| 24                   | 0.6002 |
| 32                   | 0.6438 |
| 40                   | 0.7111 |
| 44                   | 0.7646 |
| 48                   | 0.8467 |

The eigenvalues of PCA indicate how much variance can be explained by its associated eigenvectors, with the highest eigenvalue indicating the highest variance in the data. By summing up all eigenvalues, we can compute the explained variance ratio and see how the retained variance affects the performance of the model.

![](/img/week4/pca-retained-variance.png "AUC vs Retained Variance")

From the plot, we can see that we require all of the components (48), or most of the variance, in order to achieve the best performance.

## T-distributed Stochastic Neighbor Embedding (T-SNE)

[T-distributed Stochastic Neighbor Embedding (T-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) is another dimensionality reduction technique that is quite popularly used to visualize high-dimensional data in 2D. We employ t-SNE of the patient data to models the high-dimensional data into a two-dimensional embedding such that similar patients are represented as nearby points and dissimilar points as distant points.

![](/img/week4/t-sne.png "t-SNE")

With t-SNE, the visualization is much easier to see as the data points are more dispersed throughout the plot compared to PCA. It would be interesting to see if clusters would form for patients with similar diseases and symptoms.

## Next up

In the upcoming week, we'll explore feature selection techniques such as backward and forward feature selection to reduce the dimensionality of our data, yet still obtain good results. We'll look into hyperparameter tuning for our logistic regression, linear and kernel SVMs and random forest to obtain the optimal performance and results. Finally, we'll look into adding [ICD-9](https://en.wikipedia.org/wiki/ICD) codes, which is a system used by physicians and other healthcare providers to classify diseases, then evaluate and compare our results with [ICU scoring systems](https://en.wikipedia.org/wiki/ICU_scoring_systems).
