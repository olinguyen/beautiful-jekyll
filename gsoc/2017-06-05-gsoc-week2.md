---
layout: post
title: Google Summer of Code - Week 2
# bigimg: /img/gsoc-logo.png
tags: [gsoc, machine-learning]
---

Last week, I built simple models using logistic regression, SVMs and random forest for mortality prediction and hospital length of stay for patients in the ICU using the MIMIC database. To remind ourselves of what the MIMIC database looks like, I've made a table and some plots below to have a better idea of what data we're working with. The code that goes along side with this blog can be found [here](https://github.com/olinguyen/gsoc2017-shogun-dataproject).

| Information                    | Totals |
|--------------------------------|-------------|
| Age, years, median             | 65.769      |
| Gender, male (%)               | 56.207      |
| Distinct number of patients    | 38,597      |
| Distinct ICU stays             | 53,423      |
| Hospital admissions            | 49,785      |
| Hospital length of stay (days) | 11.545      |
| ICU length of stay (days)      | 2.144       |
| Hospital mortality (%)         | 11.545      |
| ICU mortality (%)              | 8.545       |

Since a patient can be admitted to the hospital multiple times, and from there can enter the ICU also more than once, there are more ICU stays than hospital admissions and patients.

![](/img/week2/hist-mimic.png "Histograms for MIMIC")

From the plots, we can see that most patients that passed away were aged above 50 years old, and that patients usually don't stay longer than 40 days in the hospital. Patients aged over 89 have a fixed value over 300 and will have to be cleaned properly.  

## Preliminary Models

For the first model, I used simple clinical data, demographic information and basic vital signs, which are summarized in the tables below.

| Demographic & Clinical Info             | Description |
|-------------------------|---------------------------------------------|
| Age                     | Age of the patient upon entering the ICU    |
| Gender                  | Patient gender (male or female)             |
| Hospital length of stay | Number of days spent in the hospital        |
| ICU length of stay      | Number of days spent in the ICU             |
| First care unit         | ICU type in which the patient was cared for |

The minimum and maximum value are both taken for the follow vital signs over the first 24 hours of the ICU stay.


| Vital Sign               | Description |
|--------------------------|-------------|
| Heart Rate               | Heartbeat rate of the patient |
| Mean Blood Pressure      | Average pressure in a patient's arteries during one cardiac cycle       |
| Respiratory Rate         | Number of breaths taken per minute       |

Despite using very basic characteristics, the models for mortality prediction of last week seem to produce good results. Upon digging deeper, I realized that it was because I was using the vital signs for the entire hospital admission as opposed to only the first 24 hours of an ICU stay. Including vital signs for the entire hospital most likely included the vital signs at the moment right before the patient passed away (if it was the case), which probably explains why the prediction results are so high with very simple features; the vital signs of a dying person is likely significantly different than the normal behaviour. Because the focus should be on the first 24 hours in the ICU so that we can improve decision-making and do patient monitoring as early as possible, the database query had to be modified to suit this criteria. A side-by-side comparison is shown between obtaining vital signs from an entire hospital admission, and only the first 24 hours. It can be observed that the prediction task is much more difficult when only looking at the first ICU stay.

| Classifier | Mean AUC across 10 folds (%)  |
|--|
|                     | Entire Admission | First 24 hours of the ICU stay |
| Linear SVM          | 77.088       |   69.662 |
| Kernel SVM (RBF)    | 75.734       |   62.891 |
| Logistic Regression | 77.539       |   69.379 |
| Random Guess        | 49.70        |   50.961 |

The same was done for predicting the hospital length of stay. Again, the results with the new query using only the first 24 hours of the ICU stay yield a lower performance.

| Classifier | Mean Squared Error |
| - |
| | Entire Admission | First 24 hours of the ICU stay |
| Least Squares Regression | 76.587   |  115.458 |
| Linear Ridge Regression  | 76.588   |  115.458 |

In the figure below, the performance results from different models are compared using a boxplot so that variability can also be observed across all folds from [stratified 10-fold cross-validation]([1]). Logistic regression and linear SVM produced the best results with around 69% auROC, while random forest performed the worst hovering above 50%. However, these results could be improved with hyper-parameter tuning.

![](/img/week2/boxplot-mp-new.png "Mortality Prediction Box plot")
![](/img/week2/boxplot-los-new.png "Hospital Length of Stay Box plot")

## Next Up

In the upcoming weeks, I'll be looking into improving the models even further. I'll try to include laboratory measurements (fluids taken from a patients body and analyzed in the lab to obtain glucose, potassium, etc). In addition, diagnoses and symptoms are also recorded using ICD-9 codes, which is a system used by physicians and other healthcare providers to classify diseases. These additional features would likely improve the predictive capabilities of the current models. Finally, I'll attempt to better interpret the results of the models e.g. try to understand which features are good indicators of an outcome, identify where the models perform well and has more trouble.

[1]: (https://en.wikipedia.org/wiki/Cross-validation_\(statisticsâ€º)#k-fold_cross-validation)
